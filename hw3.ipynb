{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "30174497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import math\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "add6bf93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tune_bb(algo, X, y, \n",
    "            regularization=\"Dropout\", M=10, \n",
    "            c=None, K=5, criterion=\"MSE\"):\n",
    "\n",
    "    \"\"\"function to automatically tune blackbox regression model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "\n",
    "    algo : callable\n",
    "        A learning algorithm that takes as input a matrix X in R nxp\n",
    "        and a vector of responses Y in Rn and returns a function that\n",
    "        maps inputs to outputs. Must have methods like .fit() and .predict()\n",
    "    X : array-like of shape (n,p)\n",
    "        training data X in R nxp\n",
    "    y : array-like of shape (n,)\n",
    "        training labels, Y, in Rn\n",
    "    regularization : str, default=\"Dropout\"\n",
    "        regularization method, can be any of \"Dropout\",\n",
    "        \"NoiseAddition\", or \"Robust\"\n",
    "    M : int, default=\n",
    "        A positive integer indicating the number of Monte Carlo\n",
    "        replicates to be used if the method specified is Dropout or \n",
    "        NoiseAddition\n",
    "    c : default=None\n",
    "        A vector of column bounds to be used if method specified is \"Robust\"\n",
    "    K : int, default=5\n",
    "        A positive integer indicating the number of CV-folds to be used to \n",
    "        tune the amount of regularization, e.g., K = 5 indicates five-fold CV\n",
    "    criterion : str, default=\"MSE\"\n",
    "        A criterion to be used to evaluate the method that belongs to the set \n",
    "        {MSE, MAD} where MSE encodes mean square error and MAD encodes mean\n",
    "        absolute deviation.\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    tuned_mode : callable \n",
    "        A tuned predictive model that optimizes the specific criterion using \n",
    "        the specified method\n",
    "\n",
    "    Example:\n",
    "    -----------\n",
    "    >>> tune_bb()\n",
    "    >>> \n",
    "    \"\"\"\n",
    "    \n",
    "    # statements here to ensure model has the methods we need to tune it\n",
    "    assert hasattr(algo, \"fit\"), \"model object must have .fit() method\"\n",
    "    assert hasattr(algo, \"predict\"), \"model object must have .predict() method\"\n",
    "\n",
    "    if criterion == \"MSE\":\n",
    "        criterion = \"neg_mean_squared_error\"\n",
    "    elif criterion == \"MAE\":\n",
    "        criterion = \"neg_mean_absolute_error\"\n",
    "    else:\n",
    "        raise ValueError(\"Please input either MAE or MSE for criterion.\")\n",
    "    \n",
    "    # Standardize X (useful for all methods with regularization)\n",
    "    scaler = StandardScaler()  \n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    if regularization == \"Dropout\":\n",
    "        # dropout notes: topic 1 p. 30 \n",
    "        # draw Z matrix\n",
    "        Z = np.random.binomial(1,0.5,size=X.shape)\n",
    "        dropout_matrix = Z * X \n",
    "        \n",
    "    elif regularization == \"NoiseAddition\":\n",
    "        # possible levels of noise\n",
    "        lambda_levels = np.linspace(0, 5, 100)\n",
    "        min_metric = None\n",
    "        best_lambda = None\n",
    "        for lam in lambda_levels: \n",
    "            #CV\n",
    "            metric = []\n",
    "            for m in range(M):\n",
    "                # generate noise matrix with lambda (variance, not std)\n",
    "                Z = np.random.normal(0, lam**2, size=X.shape)\n",
    "                kf = KFold(n_splits=K)\n",
    "                for train_index, test_index in kf.split(X):\n",
    "                    X_train, y_train = X[train_index], y[train_index]\n",
    "                    X_test, y_test = X[test_index], y[test_index]\n",
    "                    X_train_disturbed = X_train + Z[train_index]\n",
    "                    model = algo\n",
    "                    model.fit(X_train_disturbed, y_train)\n",
    "                    if criterion == \"neg_mean_squared_error\":\n",
    "                        metric.append(mean_squared_error(y_test, model.predict(X_test)))\n",
    "                    else:\n",
    "                        metric.append(mean_absolute_error(y_test, model.predict(X_test)))\n",
    "            new_metric = np.mean(metric)\n",
    "            if (min_metric == None or new_metric < min_metric):\n",
    "                best_lambda = lam\n",
    "                min_metric = new_metric\n",
    "                                  \n",
    "        print('best lambda')\n",
    "        print(best_lambda)\n",
    "        \n",
    "    elif regularization == \"Robust\":\n",
    "        #Assertion: we don't actually have any reasonable bounds to argue for robust regression here\n",
    "        #We could take a sample of many matrix M and then choose 1 based on a criteria to fit our regression\n",
    "        #If we do this iteratively with changing our c bounds, we can theoretically narrow down our bounds wrt the MSE/MAE\n",
    "        tol = False #Are we in our tolerance range\n",
    "        toler = 5e-4 #Sklearn default for tolerance is 1e-4\n",
    "        wts = [x/2 for x in c] #Initial weights are going to be c/2\n",
    "        oerror = np.inf #Initial error to not get in the way\n",
    "        merror = np.inf #Initialize new error so object exists\n",
    "        itera = 0 #We want to be able to count the number of iterations (since the weights scale by e^-itera\n",
    "        while not tol:\n",
    "            print(itera, merror, wts) #Print lines I was using to track what's going on\n",
    "            #Create a bunch of matrices and choose the best by some score\n",
    "            maxmatrix = None\n",
    "            maxnorm = -np.inf\n",
    "            for i in range(1000):\n",
    "                matrix = np.random.rand(X.shape[0], X.shape[1])\n",
    "                for m in range(matrix.shape[1]):\n",
    "                    matrix[:,m] = (wts[m] / np.linalg.norm(matrix[:,m], 2)) * matrix[:,m]\n",
    "                fnorm = np.linalg.norm(matrix, 2) #The criteria I'm using here is the two-norm\n",
    "                if fnorm > maxnorm:\n",
    "                    maxnorm = fnorm\n",
    "                    maxmatrix = matrix\n",
    "            new_X = X + maxmatrix #We add the permuted matrix to our design matrix\n",
    "            \n",
    "            #kfold cross validation\n",
    "            errors = np.abs(cross_val_score(algo, new_X, y, cv=K, scoring=criterion))\n",
    "            merror = np.mean(errors)\n",
    "            #I originally thought about this being only if the error was decreasing\n",
    "            #The problem with that is that we're dependent a random subset of generated matrix or we could get stuck in a lucky minimum\n",
    "            #Hence I think it's better to allow converence to a step to step change, even if its not at our minimum MSE\n",
    "            if abs(oerror - merror) > toler:\n",
    "                print(abs(oerror - merror)) #See if we're converging correctly\n",
    "                oerror = merror #Save the current error for the next iteration\n",
    "                #Set our new weights for the next iteration\n",
    "                wts = np.minimum(wts + (np.random.normal(size = len(wts)) * math.exp(-itera/2)), c)\n",
    "                wts = np.maximum(0.1, wts) #weights can't be negative\n",
    "                itera += 1\n",
    "            else:\n",
    "                tol = True\n",
    "        \n",
    "        #Once we have our best c bounds, let's use them exactly to construct the best model\n",
    "        maxmatrix = None\n",
    "        maxnorm = -np.inf\n",
    "        for i in range(10000):\n",
    "            matrix = np.random.rand(X.shape[0], X.shape[1])\n",
    "            for m in range(matrix.shape[1]):\n",
    "                matrix[:,m] = (wts[m] / np.linalg.norm(matrix[:,m], 2)) * matrix[:,m]\n",
    "            fnorm = np.linalg.norm(matrix, 2) #The criteria I'm using here is the two-norm\n",
    "            if fnorm > maxnorm:\n",
    "                maxnorm = fnorm\n",
    "                maxmatrix = matrix\n",
    "        new_X = X + maxmatrix #We add the permuted matrix to our design matrix\n",
    "        \n",
    "        model = algo\n",
    "        tuned_mode = model.fit(new_X, y)\n",
    "        #print(tuned_mode.coef_)\n",
    "        return tuned_mode\n",
    "    \n",
    "    else: \n",
    "        raise ValueError('Please input one of of \"Dropout\", \"NoiseAddition\", or \"Robust\"')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b601f4d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 inf [2.0, 2.0, 3.0, 1.5]\n",
      "inf\n",
      "1 0.06977271381719005 [1.19049989 2.61969236 4.00003914 2.11689383]\n",
      "0.0027639652499021894\n",
      "2 0.07253667906709224 [1.41860031 2.31808535 3.59495338 2.31971237]\n",
      "0.0024509479552507624\n",
      "3 0.074987627022343 [1.92556876 2.00596372 3.46177965 2.35165647]\n",
      "0.0022522943625744862\n",
      "4 0.07723992138491749 [1.95431057 1.68963438 3.59960962 2.50723644]\n",
      "0.002593100792406197\n",
      "5 0.07983302217732369 [1.75013398 1.71429934 3.52137313 2.68581626]\n",
      "0.0047517849757485325\n",
      "6 0.08458480715307222 [1.80451775 1.74482324 3.59974129 2.72391039]\n",
      "0.0041960007999607835\n",
      "7 0.08038880635311144 [1.85449716 1.75128116 3.52074712 2.77260743]\n",
      "0.007004344798177825\n",
      "8 0.07338446155493361 [1.85191784 1.72949086 3.54049099 2.80313851]\n",
      "0.006558509919818545\n",
      "9 0.07994297147475216 [1.84516214 1.71185383 3.51914363 2.81724051]\n",
      "0.008489208898058001\n",
      "10 0.07145376257669415 [1.85234825 1.70312722 3.53046095 2.815261  ]\n",
      "0.0052504776419727145\n",
      "11 0.07670424021866687 [1.8532827  1.70934529 3.52952753 2.8159873 ]\n",
      "0.0006256302474659248\n",
      "12 0.0773298704661328 [1.8511321  1.70718011 3.52535504 2.81019869]\n",
      "0.004132168910395945\n",
      "13 0.08146203937652874 [1.85528417 1.70790578 3.52538065 2.80814784]\n",
      "0.004282073271638584\n",
      "14 0.07717996610489015 [1.85588733 1.70475401 3.52636787 2.80719584]\n",
      "0.002621519806955172\n",
      "15 0.07980148591184533 [1.85552918 1.70451175 3.5252941  2.8075673 ]\n",
      "0.005470313728958856\n",
      "16 0.07433117218288647 [1.85492536 1.70439532 3.52461219 2.80763677]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-11 {color: black;background-color: white;}#sk-container-id-11 pre{padding: 0;}#sk-container-id-11 div.sk-toggleable {background-color: white;}#sk-container-id-11 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-11 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-11 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-11 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-11 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-11 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-11 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-11 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-11 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-11 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-11 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-11 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-11 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-11 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-11 div.sk-item {position: relative;z-index: 1;}#sk-container-id-11 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-11 div.sk-item::before, #sk-container-id-11 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-11 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-11 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-11 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-11 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-11 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-11 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-11 div.sk-label-container {text-align: center;}#sk-container-id-11 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-11 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-11\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Ridge(alpha=0.5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" checked><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge(alpha=0.5)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "Ridge(alpha=0.5)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrices to play around with and feed into function \n",
    "# NO train test split here, just full data\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "\n",
    "# test here, maybe start testing with Ridge()\n",
    "tune_bb(Ridge(alpha = 0.5),\n",
    "        X,\n",
    "        y,\n",
    "        regularization=\"Robust\",\n",
    "        c = [4,4,6,3],\n",
    "        criterion=\"MSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5dbdfd",
   "metadata": {},
   "source": [
    "### Scratchwork below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4da8686d-85b7-4d96-a77c-159154f86f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best lambda\n",
      "0.15151515151515152\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7071565656565657"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In theory, using linear regression with noise addition is equivalent to ridge\n",
    "\n",
    "tune_bb(LinearRegression(),\n",
    "        X,\n",
    "        y,\n",
    "        regularization=\"NoiseAddition\",\n",
    "        c = [2,2,1,1.5],\n",
    "        criterion=\"MSE\")\n",
    "\n",
    "# Load package\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "# Fit ridge regression using LOOCV\n",
    "alphas = np.linspace(0.0001, 5, 100)\n",
    "ridge_cv = RidgeCV(alphas = alphas).fit(X,y)\n",
    "ridge_cv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0da3ec7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe something like this if they all follow the .fit() syntax? \n",
    "def model_test(model,X,y):\n",
    "    return model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d3c9ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.11190585, -0.04007949,  0.22864503,  0.60925205])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this passes the model in ?\n",
    "model_test(LinearRegression(), X, y).coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ae78d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.11346491, -0.03184254,  0.25936799,  0.53764103])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_test(Ridge(), X, y).coef_ # niceeeeeeeeee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "417e5122",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.57894737, -0.        , -0.        ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_test(Lasso(), X, y).coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b05fbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ridge tuning parameter, for example\n",
    "# expand this later\n",
    "parameter_set = np.linspace(0,10,11) # 0 to 10\n",
    "\n",
    "cv_score_set = []\n",
    "# begin cross validation\n",
    "for alpha in parameter_set:\n",
    "\n",
    "    algo.alpha = alpha\n",
    "    cv = cross_val_score(algo, \n",
    "                         X, \n",
    "                         y, \n",
    "                         scoring=criterion,\n",
    "                         cv=K)\n",
    "\n",
    "    # cv returns negative values, need abs()\n",
    "    cv_score = np.mean(np.absolute(cv))\n",
    "    cv_score_set.append(cv_score)\n",
    "\n",
    "minimum_score = cv_score_set.index(np.min(cv_score_set))\n",
    "alpha_value = parameter_set[minimum_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41174f11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_iris, y_iris = datasets.load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39da40a3",
   "metadata": {},
   "source": [
    "### Resources and Notes: \n",
    "\n",
    "1. https://www.statology.org/k-fold-cross-validation-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "37f97403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to view doc string\n",
    "?tune_bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4dae31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
