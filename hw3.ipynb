{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30174497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import math\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "add6bf93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tune_bb(algo, X, y, regularization=\"Dropout\", M=10, c=None, K=5, criterion=\"MSE\"):\n",
    "\n",
    "    \"\"\"function to automatically tune blackbox regression model\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "\n",
    "    algo : callable\n",
    "        A learning algorithm that takes as input a matrix X in R nxp\n",
    "        and a vector of responses Y in Rn and returns a function that\n",
    "        maps inputs to outputs. Must have methods like .fit() and .predict()\n",
    "    X : array-like of shape (n,p)\n",
    "        training data X in R nxp\n",
    "    y : array-like of shape (n,)\n",
    "        training labels, Y, in Rn\n",
    "    regularization : str, default=\"Dropout\"\n",
    "        regularization method, can be any of \"Dropout\",\n",
    "        \"NoiseAddition\", or \"Robust\"\n",
    "    M : int, default=\n",
    "        A positive integer indicating the number of Monte Carlo\n",
    "        replicates to be used if the method specified is Dropout or \n",
    "        NoiseAddition\n",
    "    c : default=None\n",
    "        A vector of column bounds to be used if method specified is \"Robust\"\n",
    "    K : int, default=5\n",
    "        A positive integer indicating the number of CV-folds to be used to \n",
    "        tune the amount of regularization, e.g., K = 5 indicates five-fold CV\n",
    "    criterion : str, default=\"MSE\"\n",
    "        A criterion to be used to evaluate the method that belongs to the set \n",
    "        {MSE, MAD} where MSE encodes mean square error and MAD encodes mean\n",
    "        absolute deviation.\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    tuned_model : callable \n",
    "        A tuned predictive model that optimizes the specific criterion using \n",
    "        the specified method\n",
    "    \"\"\"\n",
    "    \n",
    "    # statements here to ensure model has the methods we need to tune it\n",
    "    assert hasattr(algo, \"fit\"), \"model object must have .fit() method\"\n",
    "    assert hasattr(algo, \"predict\"), \"model object must have .predict() method\"\n",
    "\n",
    "    if criterion == \"MSE\":\n",
    "        criterion = \"neg_mean_squared_error\"\n",
    "    elif criterion == \"MAE\":\n",
    "        criterion = \"neg_mean_absolute_error\"\n",
    "    else:\n",
    "        raise ValueError(\"Please input either MAE or MSE for criterion.\")\n",
    "    \n",
    "    # Standardize X (useful for all methods with regularization)\n",
    "    scaler = StandardScaler()  \n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    if regularization == \"Dropout\":\n",
    "        \n",
    "        # parameter to tune\n",
    "        phi_range = np.linspace(0,1,100)\n",
    "        min_metric = None\n",
    "        best_phi = None\n",
    "        for phi in phi_range:  \n",
    "            metric = []\n",
    "            # CV over K Folds\n",
    "            for m in range(M):\n",
    "                dropout_matrix = np.random.binomial(1,phi,size=X.shape) * X\n",
    "                kf = KFold(n_splits=K)\n",
    "                for train_index, test_index in kf.split(dropout_matrix):\n",
    "                        X_train, y_train = dropout_matrix[train_index], y[train_index]\n",
    "                        X_test, y_test = dropout_matrix[test_index], y[test_index]\n",
    "                        model = algo\n",
    "                        model.fit(X_train, y_train)\n",
    "                        if criterion == \"neg_mean_squared_error\":\n",
    "                            metric.append(mean_squared_error(y_test, model.predict(X_test)))\n",
    "                        else:\n",
    "                            metric.append(mean_absolute_error(y_test, model.predict(X_test)))\n",
    "                            \n",
    "            new_metric = np.mean(metric)\n",
    "            if (min_metric == None or new_metric < min_metric):\n",
    "                best_phi = phi\n",
    "                min_metric = new_metric\n",
    "        \n",
    "        # make a dropout matrix with the best choice of phi \n",
    "        dropout_matrix = np.random.binomial(1,best_phi,size=X.shape) * X\n",
    "        model = algo\n",
    "        tuned_model = model.fit(dropout_matrix, y)\n",
    "\n",
    "    elif regularization == \"NoiseAddition\":\n",
    "        # possible levels of noise\n",
    "        lambda_levels = np.linspace(0, 5, 100)\n",
    "        min_metric = None\n",
    "        best_lambda = None\n",
    "        for lam in lambda_levels: \n",
    "            #CV\n",
    "            metric = []\n",
    "            for m in range(M):\n",
    "                # generate noise matrix with lambda (variance, not std)\n",
    "                Z = np.random.normal(0, lam**2, size=X.shape)\n",
    "                kf = KFold(n_splits=K)\n",
    "                for train_index, test_index in kf.split(X):\n",
    "                    X_train, y_train = X[train_index], y[train_index]\n",
    "                    X_test, y_test = X[test_index], y[test_index]\n",
    "                    X_train_disturbed = X_train + Z[train_index]\n",
    "                    model = algo\n",
    "                    model.fit(X_train_disturbed, y_train)\n",
    "                    if criterion == \"neg_mean_squared_error\":\n",
    "                        metric.append(mean_squared_error(y_test, model.predict(X_test)))\n",
    "                    else:\n",
    "                        metric.append(mean_absolute_error(y_test, model.predict(X_test)))\n",
    "            new_metric = np.mean(metric)\n",
    "            if (min_metric == None or new_metric < min_metric):\n",
    "                best_lambda = lam\n",
    "                min_metric = new_metric\n",
    "        \n",
    "        Z = np.random.normal(0, best_lambda**2, size=X.shape)\n",
    "        new_X = X + Z\n",
    "        model = algo\n",
    "        tuned_model = model.fit(new_X, y)\n",
    "        \n",
    "    elif regularization == \"Robust\":\n",
    "        #Assertion: we don't actually have any reasonable bounds to argue for robust regression here\n",
    "        #We could take a sample of many matrix M and then choose 1 based on a criteria to fit our regression\n",
    "        #If we do this iteratively with changing our c bounds, we can theoretically narrow down our bounds wrt the MSE/MAE\n",
    "        tol = False #Are we in our tolerance range\n",
    "        toler = 5e-4 #Sklearn default for tolerance is 1e-4\n",
    "        wts = [x/2 for x in c] #Initial weights are going to be c/2\n",
    "        oerror = np.inf #Initial error to not get in the way\n",
    "        merror = np.inf #Initialize new error so object exists\n",
    "        itera = 0 #We want to be able to count the number of iterations (since the weights scale by e^-itera\n",
    "        while not tol:\n",
    "            #print(itera, merror, wts) #Print lines I was using to track what's going on\n",
    "            #Create a bunch of matrices and choose the best by some score\n",
    "            maxmatrix = None\n",
    "            maxnorm = -np.inf\n",
    "            for i in range(1000):\n",
    "                matrix = np.random.rand(X.shape[0], X.shape[1])\n",
    "                for m in range(matrix.shape[1]):\n",
    "                    matrix[:,m] = (wts[m] / np.linalg.norm(matrix[:,m], 2)) * matrix[:,m]\n",
    "                fnorm = np.linalg.norm(matrix, 2) #The criteria I'm using here is the two-norm\n",
    "                if fnorm > maxnorm:\n",
    "                    maxnorm = fnorm\n",
    "                    maxmatrix = matrix\n",
    "            new_X = X + maxmatrix #We add the permuted matrix to our design matrix\n",
    "            \n",
    "            #kfold cross validation\n",
    "            errors = np.abs(cross_val_score(algo, new_X, y, cv=K, scoring=criterion))\n",
    "            merror = np.mean(errors)\n",
    "            #I originally thought about this being only if the error was decreasing\n",
    "            #The problem with that is that we're dependent a random subset of generated matrix or we could get stuck in a lucky minimum\n",
    "            #Hence I think it's better to allow converence to a step to step change, even if its not at our minimum MSE\n",
    "            if abs(oerror - merror) > toler:\n",
    "                #print(abs(oerror - merror)) #See if we're converging correctly\n",
    "                oerror = merror #Save the current error for the next iteration\n",
    "                #Set our new weights for the next iteration\n",
    "                wts = np.minimum(wts + (np.random.normal(size = len(wts)) * math.exp(-itera/2)), c)\n",
    "                wts = np.maximum(0.1, wts) #weights can't be negative\n",
    "                itera += 1\n",
    "            else:\n",
    "                tol = True\n",
    "        \n",
    "        #Once we have our best c bounds, let's use them exactly to construct the best model\n",
    "        maxmatrix = None\n",
    "        maxnorm = -np.inf\n",
    "        for i in range(10000):\n",
    "            matrix = np.random.rand(X.shape[0], X.shape[1])\n",
    "            for m in range(matrix.shape[1]):\n",
    "                matrix[:,m] = (wts[m] / np.linalg.norm(matrix[:,m], 2)) * matrix[:,m]\n",
    "            fnorm = np.linalg.norm(matrix, 2) #The criteria I'm using here is the two-norm\n",
    "            if fnorm > maxnorm:\n",
    "                maxnorm = fnorm\n",
    "                maxmatrix = matrix\n",
    "        new_X = X + maxmatrix #We add the permuted matrix to our design matrix\n",
    "        \n",
    "        model = algo\n",
    "        tuned_model = model.fit(new_X, y)\n",
    "    else: \n",
    "        raise ValueError('Please input one of of \"Dropout\", \"NoiseAddition\", or \"Robust\"')\n",
    "\n",
    "    return tuned_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3319e632",
   "metadata": {},
   "source": [
    "### Function Demonstration\n",
    "\n",
    "Below we fit our function on the iris data and show that the three regularization methods give identical answers for the coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed959b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to view doc string\n",
    "?tune_bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b601f4d6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# getting full iris data set to train our model\n",
    "X, y = datasets.load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f630f802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robust_Ridge Regression Coefficients :  [-0.11346491 -0.03184254  0.25936799  0.53764103]\n",
      "Dropout_Ridge Regression Coefficients :  [-0.11346491 -0.03184254  0.25936799  0.53764103]\n",
      "Noise_Ridge Regression Coefficients :  [-0.11346491 -0.03184254  0.25936799  0.53764103]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Robust_Ridge = tune_bb(Ridge(),\n",
    "                       X,\n",
    "                       y,\n",
    "                       regularization=\"Robust\",\n",
    "                       c = [4,4,6,3],\n",
    "                       criterion=\"MSE\")\n",
    "\n",
    "Dropout_Ridge = tune_bb(Ridge(),\n",
    "                        X,\n",
    "                        y,\n",
    "                        regularization=\"Dropout\",\n",
    "                        criterion=\"MSE\")\n",
    "\n",
    "Noise_Ridge = tune_bb(Ridge(),\n",
    "                      X,\n",
    "                      y,\n",
    "                      regularization=\"NoiseAddition\",\n",
    "                      criterion=\"MSE\")\n",
    "\n",
    "rr = Robust_Ridge.fit(X,y)\n",
    "dr = Dropout_Ridge.fit(X,y)\n",
    "nr = Noise_Ridge.fit(X,y)\n",
    "\n",
    "print(\"Robust_Ridge Regression Coefficients : \", rr.coef_)\n",
    "print(\"Dropout_Ridge Regression Coefficients : \", dr.coef_)\n",
    "print(\"Noise_Ridge Regression Coefficients : \", nr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39da40a3",
   "metadata": {},
   "source": [
    "### Resources and Notes: \n",
    "\n",
    "1. https://www.statology.org/k-fold-cross-validation-in-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
