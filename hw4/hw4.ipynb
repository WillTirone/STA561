{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a22882a3-2af9-44ed-829d-d254864ed184",
   "metadata": {},
   "source": [
    "Authors  \n",
    " -  Alonso Guerrero Castaneda (UID: 1194613)  \n",
    " -  Eli Gnesin (UID: 1172961)  \n",
    " -  Tommy Misikoff (UID: 1166813)  \n",
    " -  Sanskriti Purohit (UID: 1179957)  \n",
    " -  Will Tirone (UID: 1130904)  \n",
    "\n",
    "TA: Rick Presman "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3a9f19",
   "metadata": {},
   "source": [
    "### Question 1 {-}\n",
    "\n",
    "Consider data ${(T_i, \\delta_i)}_{i=1}^n$ as data and consider the Kaplan-Meier estimator $\\hat{S}_n(t) = \\prod_{j:t_j < t}(1 - \\frac{d_j}{n_j})$ where $n_j$ are the number of subjects still alive at time $t_j$ and $d_j$ are the number of subjects who died at time $t_j$ and $j$ is the index of observed event times. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7f981f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Question 2 {-}\n",
    "\n",
    "Consider observed iid data ${(\\mathbf{X}_i, Y_i)}_{i=1}^n$ and with $\\lambda > 0$, define $\\hat{\\beta}_n^{\\lambda} = (\\hat{\\mathbf{u}}_n, \\hat{\\mathbf{v}}_n) = \\text{arg min}_{\\mathbf{(u,v)}} \\mathbb{P}_n \\{Y - (\\mathbf{u}\\circ\\mathbf{v})^T\\mathbf{X}\\}^2 + \\lambda||\\mathbf{u}||/2 + \\lambda||\\mathbf{v}||/2$.  \n",
    "\n",
    "Then:\n",
    "\\begin{aligned}\n",
    "\n",
    "\\end{aligned}\n",
    "\n",
    "Once we recognize that $\\hat{\\beta}_n^{\\lambda}$ is the LASSO estimator with tuning parameter $\\lambda$, we can now consider how to implement a lasso estimator from this representation. First, note that since multiplication is commutative under the real numbers, $\\mathbf{u} \\circ \\mathbf{v} = \\begin{pmatrix} u_1 * v_1 & ... & u_p * v_p \\end{pmatrix} = \\begin{pmatrix} v_1 * u_1 & ... & v_p * u_p \\end{pmatrix} = \\mathbf{v} \\circ \\mathbf{u}$, and therefore $(\\mathbf{u}\\circ\\mathbf{v})\\mathbf{X} = (\\mathbf{v}\\circ\\mathbf{u})\\mathbf{X}$. Now consider the initial definition of $\\hat{\\beta}_n^{\\lambda}$ given above. Consider an initial $\\mathbf{v}_0$. Now, we have: \n",
    "\\begin{aligned}\n",
    "\\hat{\\mathbf{u}_1} &= \\text{arg min}_{\\mathbf{u}} \\mathbb{P}_n \\{Y - (\\mathbf{u}\\circ\\mathbf{v}_0)^T\\mathbf{X}\\}^2 + \\lambda||\\mathbf{u}||/2 + \\lambda||\\mathbf{v}_0||/2 \\\\\n",
    "&= \\text{arg min}_{\\mathbf{u}} \\mathbb{P}_n \\{Y - (\\mathbf{u}\\circ\\mathbf{v}_0)^T\\mathbf{X}\\}^2 + \\lambda||\\mathbf{u}||/2 + const.\n",
    "\\end{aligned}\n",
    "\n",
    "Now, we can consider the matrix $\\mathbf{X}^*$, defined by taking the elements $v_i$ of $\\mathbf{v}_0$ and multiplying $v_i$ by the $i^{th}$ row of $\\mathbf{X}$. In this construction, $\\mathbf{u}\\mathbf{X}^* = (\\mathbf{u}\\circ\\mathbf{v}_0)\\mathbf{X}$. Then we have $\\hat{\\mathbf{u}_1} = \\text{arg min}_{\\mathbf{u}} \\mathbb{P}_n \\{Y - \\mathbf{u}^T\\mathbf{X}^*\\}^2 + \\lambda||\\mathbf{u}||/2 + const.$. Now, since we are minimizing with respect to $\\mathbf{u}$, and the constant $\\lambda||\\mathbf{v}_0||/2$ does not contain $\\mathbf{u}$, we can ignore it with respect to the minimization (since for every $\\mathbf{u}$, we would simply be adding on the same extra term). Then, we have $\\hat{\\mathbf{u}_1} = \\text{arg min}_{\\mathbf{u}} \\mathbb{P}_n \\{Y - \\mathbf{u}^T\\mathbf{X}^*\\}^2 + \\lambda||\\mathbf{u}||/2$, which is simply the equation that minimizes to Ridge Regression (with $\\lambda/2$ instead of $\\lambda$, but we could simply let $\\lambda^* = \\lambda/2$ to avoid this). As such, we have used this representation to minimize $\\mathbf{u}$ given a fixed $\\mathbf{v}$ by Ridge Regression.\n",
    "\n",
    "Now, with $\\hat{\\mathbf{u}_1}$, consider the matrix $\\mathbf{X}^{\\circ}$, defined by taking the elements $u_i$ of $\\hat{\\mathbf{u}_1}$ and multiplying $u_i$ by the $i^{th}$ row of $\\mathbf{X}$. With this construction, $\\mathbf{v}\\mathbf{X}^{\\circ} = (\\mathbf{v} \\circ \\hat{\\mathbf{u}_1})\\mathbf{X}$. Then, we can consider the minimization of $\\mathbf{v}$:\n",
    "\\begin{aligned}\n",
    "\\hat{\\mathbf{v}_1} &= \\text{arg min}_{\\mathbf{v}} \\mathbb{P}_n \\{Y - (\\mathbf{v}\\circ\\hat{\\mathbf{u}_1})^T\\mathbf{X}\\}^2 + \\lambda||\\hat{\\mathbf{u}_1}||/2 + \\lambda||\\mathbf{v}||/2 \\\\\n",
    "&= \\text{arg min}_{\\mathbf{v}} \\mathbb{P}_n \\{Y - \\mathbf{v}^T\\mathbf{X}^{\\circ}\\}^2 + \\lambda||\\mathbf{v}||/2 + \\lambda||\\hat{\\mathbf{u}_1}||/2\n",
    "\\end{aligned}\n",
    "\n",
    "Now we have $\\hat{\\mathbf{v}_1} = \\text{arg min}_{\\mathbf{v}} \\mathbb{P}_n \\{Y - \\mathbf{v}^T\\mathbf{X}^{\\circ}\\}^2 + \\lambda||\\mathbf{v}||/2 + const.$, and as with the minimization of $\\mathbf{u}$ previously, since we are minimizating with respect to $\\mathbf{v}$, $\\lambda||\\hat{\\mathbf{u}_1}||/2$ is a constant, and so we can ignore it for the sake of minimization, and thus we instead have $\\hat{\\mathbf{v}_1} = \\mathbb{P}_n \\{Y - \\mathbf{v}^T\\mathbf{X}^{\\circ}\\}^2 + \\lambda||\\mathbf{v}||/2$, which is the equation that minimizes Ridge Regression. As such, we have also minimized $\\mathbf{v}$ given $\\hat{\\mathbf{u}_1}$ by Ridge Regression.\n",
    "\n",
    "We can now repeat the above process to find $\\hat{\\mathbf{u}_i}, \\hat{\\mathbf{v}_i}$ through a series of alternating Ridge Regressions, first minimizing $\\mathbf{u}$ using $\\hat{\\mathbf{v}_{i-1}}$, and then minimizing $\\mathbf{v}$ using $\\hat{\\mathbf{u}_i}$ It is then reasonable to expect that at each iteration $i$, we would then check if $(\\hat{\\mathbf{u}_i}, \\hat{\\mathbf{v}_i})$ actually minimizes the original equation, and we would stop when the original equation was minimized (without a closed form, we could do multiple starts of the with different $\\mathbf{v}_0$ and accept the best minimized function to give us $\\hat{\\beta}_n^{\\lambda} = (\\hat{\\mathbf{u}}_n, \\hat{\\mathbf{v}}_n)$. As such, we have shown that, using this representation, we can implement lasso as a sequence of alternating Ridge Regressions using stochastic approximation methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39da40a3",
   "metadata": {},
   "source": [
    "### Resources and Notes {-}"
   ]
  }
 ],
 "metadata": {
  "date": "February 20, 2023",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "title": "STA 561 Homework 3"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
