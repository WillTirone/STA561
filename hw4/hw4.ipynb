{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a22882a3-2af9-44ed-829d-d254864ed184",
   "metadata": {},
   "source": [
    "Authors  \n",
    " -  Alonso Guerrero Castaneda (UID: 1194613)  \n",
    " -  Eli Gnesin (UID: 1172961)  \n",
    " -  Tommy Misikoff (UID: 1166813)  \n",
    " -  Sanskriti Purohit (UID: 1179957)  \n",
    " -  Will Tirone (UID: 1130904)  \n",
    "\n",
    "TA: Rick Presman "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3a9f19",
   "metadata": {},
   "source": [
    "### Question 1 {-}\n",
    "\n",
    "Consider data ${(T_i, \\delta_i)}_{i=1}^n$ as data and consider the Kaplan-Meier estimator $\\hat{S}_n(t) = \\prod_{j:t_j < t}(1 - \\frac{d_j}{n_j})$ where $n_j$ are the number of subjects still alive at time $t_j$ and $d_j$ are the number of subjects who died at time $t_j$ and $j$ is the index of observed event times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "6e7a4677-717f-4ccd-b770-3cf9712ae79a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "cannot assign to function call here. Maybe you meant '==' instead of '='? (1453190646.py, line 72)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[305], line 72\u001b[0;36m\u001b[0m\n\u001b[0;31m    local_risk_set.values() = round(local_risk_set.values())\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m cannot assign to function call here. Maybe you meant '==' instead of '='?\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "from typing import Optional\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "import tqdm\n",
    "import random\n",
    "import pdb\n",
    "\n",
    "\n",
    "def streaming_km(\n",
    "        observed_times: np.ndarray,\n",
    "        censoring_ind: np.ndarray,\n",
    "        failure_set: Optional[dict] = {},\n",
    "        risk_set: Optional[dict] = {}\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Compute the Kaplan-Meier estimator and sufficient statistics with\n",
    "    single sweep through the data\n",
    "    :param observed_times: 1D np.array of observed times, i.e., the\n",
    "        minimum of failure and censoring times\n",
    "    :param censoring_ind: 1D np.array of indicators that an observation\n",
    "        was censored\n",
    "    :param failure_set: a dictionary mapping failure times to\n",
    "        the number of observed failures at that time, if None an empty\n",
    "        dictionary is created.  This allows us to sequentially process\n",
    "        multiple datasets.  Note that we do not changed passed dictionaries.\n",
    "    :param risk_set: a dictionary mapping failure times to the\n",
    "        number of individuals at risk.\n",
    "    :return: dictionary with key --> value pairs:\n",
    "        failure_set -> dictionary with keys corresponding to unique failure\n",
    "            times and values corresponding to number of observed failures at\n",
    "            that (key) time\n",
    "        risk_set -> dictionary with keys corresponding to unique failure\n",
    "            times and values corresponding to number of individuals at risk\n",
    "            at that (key) time\n",
    "        km -> dictionary with keys corresponding to unique failure times\n",
    "            and values corresponding to Kaplan-Meier estimator of\n",
    "            survivor function.\n",
    "    \"\"\"\n",
    "    \n",
    "    local_failure_set = failure_set.copy()\n",
    "    local_risk_set = risk_set.copy()\n",
    "    res_t = []\n",
    "    res_delta = []\n",
    "    K = 10\n",
    "    for x,(t, delta) in enumerate(zip(observed_times, censoring_indicator)):\n",
    "        if x < K:\n",
    "                res_t.append(t)\n",
    "                res_delta.append(delta)\n",
    "        else:\n",
    "            s = random.randint(0,x)\n",
    "            if s<K:\n",
    "                res_t[s] = t\n",
    "                res_delta[s] = delta\n",
    "        if delta == 1:\n",
    "            if t not in local_failure_set:\n",
    "                local_failure_set[t] = 0.0\n",
    "            local_failure_set[t] += 1.0\n",
    "            if t not in local_risk_set:\n",
    "                local_risk_set[t] = 0.0\n",
    "            unique_failure_times = np.sort(list(local_failure_set.keys()))\n",
    "            t_index = np.where(unique_failure_times == t)[0][0]\n",
    "            if t_index < len(unique_failure_times) - 1: # not last\n",
    "                local_risk_set[t] += \\\n",
    "                    local_risk_set[unique_failure_times[t_index+1]]\n",
    "            else:\n",
    "                local_risk_set[t] += x*sum(res_t>=t)/len(res_t)\n",
    "        risk_times = [w for w in local_risk_set.keys() if w <= t]\n",
    "        for at_risk_time in risk_times:\n",
    "            local_risk_set[at_risk_time] += 1.0\n",
    "            \n",
    "\n",
    "    # Compute KM estimator\n",
    "    unique_failure_times = np.sort(list(local_failure_set.keys()))\n",
    "    discrete_hazard = np.array(\n",
    "        [local_failure_set[j]/local_risk_set[j] for j in unique_failure_times]\n",
    "    )\n",
    "    km = np.cumprod(1-discrete_hazard)\n",
    "    km_dict = dict(zip(unique_failure_times, km))\n",
    "    return {\"failure_set\": local_failure_set, \"risk_set\": local_risk_set,\n",
    "            \"km\": km_dict}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate right-censored data\n",
    "    nr.seed(1) # for replicability\n",
    "    n = 100\n",
    "    true_failure_times = nr.exponential(size=n)*100\n",
    "    censoring_times = nr.exponential(size=n)*100\n",
    "\n",
    "    # Note that you could do this in one step rather than two\n",
    "    observed_times = np.array(\n",
    "        [min(t, c) for t, c in zip(true_failure_times, censoring_times)]\n",
    "    )\n",
    "    censoring_indicator = np.array(\n",
    "        [t <= c for t, c in zip(true_failure_times, censoring_times)]\n",
    "    )\n",
    "    km_est = streaming_km(observed_times, censoring_indicator)\n",
    "    plt.plot(km_est[\"km\"].keys(), km_est[\"km\"].values())\n",
    "    ecdf = ECDF(true_failure_times)(\n",
    "        np.linspace(min(observed_times), max(observed_times))\n",
    "    )\n",
    "    plt.plot(np.linspace(min(observed_times), max(observed_times)), 1-ecdf)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "b6e1966b-8332-46bf-ade1-f1f1cc0f5422",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'failure_set': {0.011438135864308592: 1.0,\n",
       "  36.0012754853919: 1.0,\n",
       "  15.87095951946739: 1.0,\n",
       "  9.688387165373346: 1.0,\n",
       "  42.39764818070748: 1.0,\n",
       "  50.54525417107485: 1.0,\n",
       "  77.39597748607572: 1.0,\n",
       "  22.872440752683033: 1.0,\n",
       "  2.776962476842335: 1.0,\n",
       "  111.00803264966326: 1.0,\n",
       "  54.00910464444293: 1.0,\n",
       "  81.80073139030377: 1.0,\n",
       "  15.127291958193561: 1.0,\n",
       "  8.887953331090477: 1.0,\n",
       "  3.983787812260575: 1.0,\n",
       "  10.352534930954075: 1.0,\n",
       "  54.663869952486706: 1.0,\n",
       "  117.72565948999085: 1.0,\n",
       "  115.99588780254496: 1.0,\n",
       "  1.8457575175565604: 1.0,\n",
       "  34.759365873452886: 1.0,\n",
       "  33.93618816477483: 1.0,\n",
       "  1.9556954503914579: 1.0,\n",
       "  113.58019286201848: 1.0,\n",
       "  23.778536644001125: 1.0,\n",
       "  5.483909448914068: 1.0,\n",
       "  15.867758152617037: 1.0,\n",
       "  88.99057397352507: 1.0,\n",
       "  120.31676619431752: 1.0,\n",
       "  10.795769521944356: 1.0,\n",
       "  53.45310363237981: 1.0,\n",
       "  53.474145690277886: 1.0,\n",
       "  5.124430500448381: 1.0,\n",
       "  14.789080198059363: 1.0,\n",
       "  14.99817868756233: 1.0,\n",
       "  50.69611620686383: 1.0,\n",
       "  18.074783228852244: 1.0,\n",
       "  139.00712205921738: 1.0,\n",
       "  97.20564341878024: 1.0,\n",
       "  12.188063895034396: 1.0,\n",
       "  59.76772562062613: 1.0,\n",
       "  86.36736482517783: 1.0,\n",
       "  52.44797565479637: 1.0,\n",
       "  27.053260906079878: 1.0,\n",
       "  0.287445431946199: 1.0,\n",
       "  96.00987259443872: 1.0},\n",
       " 'risk_set': {0.011438135864308592: 100.0,\n",
       "  36.0012754853919: 43.25,\n",
       "  15.87095951946739: 63.25,\n",
       "  9.688387165373346: 77.25,\n",
       "  42.39764818070748: 40.75,\n",
       "  50.54525417107485: 36.77777777777778,\n",
       "  77.39597748607572: 25.9,\n",
       "  22.872440752683033: 57.25,\n",
       "  2.776962476842335: 89.25,\n",
       "  111.00803264966326: 12.0,\n",
       "  54.00910464444293: 30.9,\n",
       "  81.80073139030377: 24.0,\n",
       "  15.127291958193561: 65.25,\n",
       "  8.887953331090477: 78.25,\n",
       "  3.983787812260575: 87.25,\n",
       "  10.352534930954075: 73.25,\n",
       "  54.663869952486706: 29.9,\n",
       "  117.72565948999085: 8.4,\n",
       "  115.99588780254496: 9.4,\n",
       "  1.8457575175565604: 92.25,\n",
       "  34.759365873452886: 44.25,\n",
       "  33.93618816477483: 45.25,\n",
       "  1.9556954503914579: 90.25,\n",
       "  113.58019286201848: 10.4,\n",
       "  23.778536644001125: 51.25,\n",
       "  5.483909448914068: 83.25,\n",
       "  15.867758152617037: 64.25,\n",
       "  88.99057397352507: 20.0,\n",
       "  120.31676619431752: 3.0,\n",
       "  10.795769521944356: 69.25,\n",
       "  53.45310363237981: 32.9,\n",
       "  53.474145690277886: 31.9,\n",
       "  5.124430500448381: 84.25,\n",
       "  14.789080198059363: 67.25,\n",
       "  14.99817868756233: 66.25,\n",
       "  50.69611620686383: 34.9,\n",
       "  18.074783228852244: 59.25,\n",
       "  139.00712205921738: 2.0,\n",
       "  97.20564341878024: 13.0,\n",
       "  12.188063895034396: 68.25,\n",
       "  59.76772562062613: 26.9,\n",
       "  86.36736482517783: 21.0,\n",
       "  52.44797565479637: 33.9,\n",
       "  27.053260906079878: 46.25,\n",
       "  0.287445431946199: 93.25,\n",
       "  96.00987259443872: 14.0},\n",
       " 'km': {0.011438135864308592: 0.99,\n",
       "  0.287445431946199: 0.9793833780160858,\n",
       "  1.8457575175565604: 0.9687667560321717,\n",
       "  1.9556954503914579: 0.9580324983476046,\n",
       "  2.776962476842335: 0.9472982406630377,\n",
       "  3.983787812260575: 0.9364409542371003,\n",
       "  5.124430500448381: 0.9253259280740487,\n",
       "  5.483909448914068: 0.9142109019109971,\n",
       "  8.887953331090477: 0.9025276954967991,\n",
       "  9.688387165373346: 0.8908444890826012,\n",
       "  10.352534930954075: 0.8786827895729411,\n",
       "  10.795769521944356: 0.8659942294347037,\n",
       "  12.188063895034396: 0.8533056692964663,\n",
       "  14.789080198059363: 0.8406171091582288,\n",
       "  14.99817868756233: 0.8279285490199915,\n",
       "  15.127291958193561: 0.8152399888817541,\n",
       "  15.867758152617037: 0.8025514287435167,\n",
       "  15.87095951946739: 0.7898628686052793,\n",
       "  18.074783228852244: 0.7765318497258653,\n",
       "  22.872440752683033: 0.7629679746214834,\n",
       "  23.778536644001125: 0.748080794628869,\n",
       "  27.053260906079878: 0.7319060747450016,\n",
       "  33.93618816477483: 0.7157313548611342,\n",
       "  34.759365873452886: 0.6995566349772667,\n",
       "  36.0012754853919: 0.6833819150933993,\n",
       "  42.39764818070748: 0.666611806747549,\n",
       "  50.54525417107485: 0.6484864101894586,\n",
       "  50.69611620686383: 0.6299051376911933,\n",
       "  52.44797565479637: 0.611323865192928,\n",
       "  53.45310363237981: 0.5927425926946628,\n",
       "  53.474145690277886: 0.5741613201963974,\n",
       "  54.00910464444293: 0.5555800476981321,\n",
       "  54.663869952486706: 0.5369987751998668,\n",
       "  59.76772562062613: 0.5170359954526599,\n",
       "  77.39597748607572: 0.49707321570545293,\n",
       "  81.80073139030377: 0.4763618317177257,\n",
       "  86.36736482517783: 0.45367793496926256,\n",
       "  88.99057397352507: 0.4309940382207994,\n",
       "  96.00987259443872: 0.4002087497764566,\n",
       "  97.20564341878024: 0.3694234613321138,\n",
       "  111.00803264966326: 0.338638172887771,\n",
       "  113.58019286201848: 0.3060768101101007,\n",
       "  115.99588780254496: 0.2735154473324304,\n",
       "  117.72565948999085: 0.24095408455476008,\n",
       "  120.31676619431752: 0.16063605636984007,\n",
       "  139.00712205921738: 0.08031802818492004}}"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km_est"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7f981f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Question 2 {-}\n",
    "\n",
    "#### $\\hat{\\beta}_n^{\\lambda}$ as the LASSO Estimator\n",
    "\n",
    "Consider observed iid data ${(\\mathbf{X}_i, Y_i)}_{i=1}^n$ and with $\\lambda > 0$, define $(\\hat{\\mathbf{u}}_n, \\hat{\\mathbf{v}}_n) = \\text{arg min}_{\\mathbf{(u,v)}} \\mathbb{P}_n \\{Y - (\\mathbf{u}\\circ\\mathbf{v})^T\\mathbf{X}\\}^2 + \\lambda||\\mathbf{u}||^2/2 + \\lambda||\\mathbf{v}||^2/2$ with $\\hat{\\beta}_n^{\\lambda} = \\hat{\\mathbf{u}}_n \\circ \\hat{\\mathbf{v}}_n$. Now, we note that since $\\circ$ is the element-wise multiplication of vectors, we can also consider the symbol $/$ to be the element-wise division of vectors, such that $\\mathbf{u} = \\beta / \\mathbf{v}$.\n",
    "\n",
    "Then:\n",
    "\\begin{aligned}\n",
    "(\\hat{\\mathbf{u}}_n, \\hat{\\mathbf{v}}_n) &= \\text{arg min}_{\\mathbf{(u,v)}} \\mathbb{P}_n \\{Y - (\\mathbf{u}\\circ\\mathbf{v})^T\\mathbf{X}\\}^2 + \\lambda||\\mathbf{u}||^2/2 + \\lambda||\\mathbf{v}||^2/2 \\\\\n",
    "&= \\text{min}_{\\mathbf{\\beta, v}} \\mathbb{P}_n \\{Y - ((\\beta / \\mathbf{v}) \\circ\\mathbf{v})^T\\mathbf{X}\\}^2 + \\lambda||\\beta / \\mathbf{v}||^2/2 + \\lambda||\\mathbf{v}||^2/2 \\\\\n",
    "&= \\text{min}_{\\mathbf{\\beta, v}} \\mathbb{P}_n \\{Y - \\beta^T\\mathbf{X}\\}^2 + \\lambda||\\beta / \\mathbf{v}||^2/2 + \\lambda||\\mathbf{v}||^2/2\n",
    "\\end{aligned}\n",
    "\n",
    "Now, rather than minimize both $\\beta$ and $\\mathbf{v}$ simultaneously, we can first consider minimizing over $\\mathbf{v}$ and then over $\\beta$:\n",
    "\\begin{aligned}\n",
    "&= \\text{min}_{\\beta} (\\text{min}_{\\mathbf{v}} \\mathbb{P}_n \\{Y - \\beta^T\\mathbf{X}\\}^2 + \\lambda||\\beta / \\mathbf{v}||^2/2 + \\lambda||\\mathbf{v}||^2/2) \\\\\n",
    "\\end{aligned}\n",
    "\n",
    "Now, when we take the derivative with respect to $\\mathbf{v}$, then $\\mathbb{P}_n \\{Y - \\beta^T\\mathbf{X}\\}^2$ does not depend on $\\mathbf{v}$ and goes to zero. Then:\n",
    "\\begin{aligned}\n",
    "\\frac{\\delta}{\\delta \\mathbf{v}} \\mathbb{P}_n \\{Y - \\beta^T\\mathbf{X}\\}^2 + \\lambda||\\beta / \\mathbf{v}||^2/2 + \\lambda||\\mathbf{v}||^2/2 &= \n",
    "\\end{aligned}\n",
    "\n",
    "#### Alternating Ridge Regression\n",
    "\n",
    "Once we recognize that $\\hat{\\beta}_n^{\\lambda}$ is the LASSO estimator with tuning parameter $\\lambda$, we can now consider how to implement a lasso estimator from this representation. First, note that since multiplication is commutative under the real numbers, $\\mathbf{u} \\circ \\mathbf{v} = \\begin{pmatrix} u_1 * v_1 & ... & u_p * v_p \\end{pmatrix} = \\begin{pmatrix} v_1 * u_1 & ... & v_p * u_p \\end{pmatrix} = \\mathbf{v} \\circ \\mathbf{u}$, and therefore $(\\mathbf{u}\\circ\\mathbf{v})\\mathbf{X} = (\\mathbf{v}\\circ\\mathbf{u})\\mathbf{X}$. Now consider the initial definition of the minimization function given above. Consider an initial $\\mathbf{v}_0$. Now, we have: \n",
    "\\begin{aligned}\n",
    "\\hat{\\mathbf{u}_1} &= \\text{arg min}_{\\mathbf{u}} \\mathbb{P}_n \\{Y - (\\mathbf{u}\\circ\\mathbf{v}_0)^T\\mathbf{X}\\}^2 + \\lambda||\\mathbf{u}||^2/2 + \\lambda||\\mathbf{v}_0||^2/2 \\\\\n",
    "&= \\text{arg min}_{\\mathbf{u}} \\mathbb{P}_n \\{Y - (\\mathbf{u}\\circ\\mathbf{v}_0)^T\\mathbf{X}\\}^2 + \\lambda||\\mathbf{u}||^2/2 + const.\n",
    "\\end{aligned}\n",
    "\n",
    "Now, we can consider the matrix $\\mathbf{X}^*$, defined by taking the elements $v_i$ of $\\mathbf{v}_0$ and multiplying $v_i$ by the $i^{th}$ row of $\\mathbf{X}$. In this construction, $\\mathbf{u}\\mathbf{X}^* = (\\mathbf{u}\\circ\\mathbf{v}_0)\\mathbf{X}$. Then we have $\\hat{\\mathbf{u}_1} = \\text{arg min}_{\\mathbf{u}} \\mathbb{P}_n \\{Y - \\mathbf{u}^T\\mathbf{X}^*\\}^2 + \\lambda||\\mathbf{u}||^2/2 + const.$. Now, since we are minimizing with respect to $\\mathbf{u}$, and the constant $\\lambda||\\mathbf{v}_0||^2/2$ does not contain $\\mathbf{u}$, we can ignore it with respect to the minimization (since for every $\\mathbf{u}$, we would simply be adding on the same extra term). Then, we have $\\hat{\\mathbf{u}_1} = \\text{arg min}_{\\mathbf{u}} \\mathbb{P}_n \\{Y - \\mathbf{u}^T\\mathbf{X}^*\\}^2 + \\lambda||\\mathbf{u}||^2/2$, which is simply the equation that minimizes to Ridge Regression (with $\\lambda/2$ instead of $\\lambda$, but we could simply let $\\lambda^* = \\lambda/2$ to avoid this). As such, we have used this representation to minimize $\\mathbf{u}$ given a fixed $\\mathbf{v}$ by Ridge Regression.\n",
    "\n",
    "Now, with $\\hat{\\mathbf{u}_1}$, consider the matrix $\\mathbf{X}^{\\circ}$, defined by taking the elements $u_i$ of $\\hat{\\mathbf{u}_1}$ and multiplying $u_i$ by the $i^{th}$ row of $\\mathbf{X}$. With this construction, $\\mathbf{v}\\mathbf{X}^{\\circ} = (\\mathbf{v} \\circ \\hat{\\mathbf{u}_1})\\mathbf{X}$. Then, we can consider the minimization of $\\mathbf{v}$:\n",
    "\\begin{aligned}\n",
    "\\hat{\\mathbf{v}_1} &= \\text{arg min}_{\\mathbf{v}} \\mathbb{P}_n \\{Y - (\\mathbf{v}\\circ\\hat{\\mathbf{u}_1})^T\\mathbf{X}\\}^2 + \\lambda||\\hat{\\mathbf{u}_1}||^2/2 + \\lambda||\\mathbf{v}||^2/2 \\\\\n",
    "&= \\text{arg min}_{\\mathbf{v}} \\mathbb{P}_n \\{Y - \\mathbf{v}^T\\mathbf{X}^{\\circ}\\}^2 + \\lambda||\\mathbf{v}||^2/2 + \\lambda||\\hat{\\mathbf{u}_1}||^2/2\n",
    "\\end{aligned}\n",
    "\n",
    "Now we have $\\hat{\\mathbf{v}_1} = \\text{arg min}_{\\mathbf{v}} \\mathbb{P}_n \\{Y - \\mathbf{v}^T\\mathbf{X}^{\\circ}\\}^2 + \\lambda||\\mathbf{v}||^2/2 + const.$, and as with the minimization of $\\mathbf{u}$ previously, since we are minimizating with respect to $\\mathbf{v}$, $\\lambda||\\hat{\\mathbf{u}_1}||^2/2$ is a constant, and so we can ignore it for the sake of minimization, and thus we instead have $\\hat{\\mathbf{v}_1} = \\mathbb{P}_n \\{Y - \\mathbf{v}^T\\mathbf{X}^{\\circ}\\}^2 + \\lambda||\\mathbf{v}||^2/2$, which is the equation that minimizes Ridge Regression. As such, we have also minimized $\\mathbf{v}$ given $\\hat{\\mathbf{u}_1}$ by Ridge Regression.\n",
    "\n",
    "We can now repeat the above process to find $\\hat{\\mathbf{u}_i}, \\hat{\\mathbf{v}_i}$ through a series of alternating Ridge Regressions, first minimizing $\\mathbf{u}$ using $\\hat{\\mathbf{v}_{i-1}}$, and then minimizing $\\mathbf{v}$ using $\\hat{\\mathbf{u}_i}$ It is then reasonable to expect that at each iteration $i$, we would then check if $(\\hat{\\mathbf{u}_i}, \\hat{\\mathbf{v}_i})$ actually minimizes the original equation, and we would stop when the original equation was minimized (without a closed form, we could do multiple starts of the with different $\\mathbf{v}_0$ and accept the best minimized function to give us $(\\hat{\\mathbf{u}}_n, \\hat{\\mathbf{v}}_n)$, which can then be used to calculate $\\hat{\\beta}_n^{\\lambda}$. As such, we have shown that, using this representation, we can implement lasso as a sequence of alternating Ridge Regressions using stochastic approximation methods.\n",
    "\n",
    "It is important to note that the above method does not include any dampening. We could choose to include dampening, within the above framework, by initializing $\\mathbf{u}_0, \\mathbf{v}_0$ and then, for any iteration $i$, calculating $\\mathbf{u}_i = \\mathbf{u}_{i-1} + \\alpha_i \\mathbf{u}^*$ where $\\mathbf{u}^*$ is the solution of the minimization with respect to $\\mathbf{u}$ using $\\mathbf{v}_{i-1}$, and then calculating $\\mathbf{v}_i = \\mathbf{v}_{i-1} + \\alpha_i \\mathbf{v}^*$ where $\\mathbf{v}^*$ is the solution of the minimization with respect to $\\mathbf{v}$ using $\\mathbf{u}_i$, with decreasing step sizes $\\alpha_i$ that satisfy the conditions $\\sum_{i \\ge 1} \\alpha_i = \\infty$ and $\\sum_{i \\ge 1} \\alpha_i^2 < \\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39da40a3",
   "metadata": {},
   "source": [
    "### Resources and Notes {-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa5ede0-1b13-4e9c-823b-7a134b5cd85e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "date": "February 20, 2023",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "title": "STA 561 Homework 3"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
