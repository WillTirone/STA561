{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a22882a3-2af9-44ed-829d-d254864ed184",
   "metadata": {},
   "source": [
    "# STA 561 HW 5 {-}\n",
    "\n",
    "Authors  \n",
    " -  Alonso Guerrero Castaneda (UID: 1194613)  \n",
    " -  Eli Gnesin (UID: 1172961)  \n",
    " -  Tommy Misikoff (UID: 1166813)  \n",
    " -  Sanskriti Purohit (UID: 1179957)  \n",
    " -  Will Tirone (UID: 1130904)  \n",
    "\n",
    "TA: Rick Presman "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40425010-82dc-454d-a370-2f6733bd33fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy.random as nr\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3a9f19",
   "metadata": {},
   "source": [
    "### Question 1 {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05458358",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}||k^\\frac{-1}{2} \\Omega \\bf{x}||^2 &= k^{-1} \\mathbb{E} \\bigg| \\bigg| \\begin{bmatrix}\n",
    "           \\Omega_{1}^T \\bf{x} \\\\\n",
    "           \\vdots \\\\\n",
    "           \\Omega_{k}^T \\bf{x}\n",
    "         \\end{bmatrix}\n",
    "\\bigg| \\bigg|^2 \\\\\n",
    "&= k^{-1} \\mathbb{E} \\bigg[ \\bigg(\\Omega_{1,1} x_1 + \\dots + \\Omega_{1,k}x_p\\bigg)^2 + \\dots + \\bigg(\\Omega_{k,1} x_1 + \\dots + \\Omega_{k,p}x_p\\bigg)^2 \\bigg] \\\\\n",
    "&= k^{-1} \\mathbb{E} \\bigg[ [ x_1^2 \\Omega_{1,1}^2 + \\Omega_{1,1}\\Omega_{1,2}x_1x_2 + \\dots] + \\dots + [x_1^2 \\Omega_{k,1}^2 + \\dots] \\bigg] \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Now the non-squared terms will be 0, since $\\mathbb{E}(\\Omega_{i,j}) = 0$ and they are i.i.d. For example, $\\mathbb{E}(\\Omega_{1,1}\\Omega_{1,2}x_1x_2) = x_1x_2\\mathbb{E} (\\Omega_{1,1})\\mathbb{E}(\\Omega_{1,2}) = 0$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&= k^{-1}  \\bigg[ [x_1^2 \\mathbb{E}(\\Omega_{1,1}^2)  + \\dots + x_p^2\\mathbb{E}(\\Omega_{1,k}^2)] + \\dots + [x_1^2 \\mathbb{E}(\\Omega_{k,1}^2) + \\dots + x_p^2\\mathbb{E}(\\Omega_{k,p}^2)] \\bigg] \\\\ \n",
    "&= k^{-1} \\bigg[ [x_1^2  + \\dots + x_p^2] + \\dots + [x_1^2 + \\dots + x_p^2 ] \\bigg] \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{second moment: } \\text{Var}(\\Omega_{i,j}) = E(\\Omega_{i,j}^2) - 0 = 1 \\\\ \n",
    "&= k^{-1} \\bigg[ || x ||^2 + \\dots + || x ||^2 \\bigg] \\\\ \n",
    "&= ||x||^2 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "If we want to generate $\\Omega_{i,j} \\overset{IID}{\\sim} Q$, the necessary conditions for the above proof to hold are that $Q$ has a mean of 0 and a variance of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7f981f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Question 2 {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe1a404",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{\\beta}_n^\\Omega &= \\Omega^T \\text{argmin}_\\beta \\mathbb{P}_n (Y - (\\Omega\\bf{X})^T\\beta)^2\\\\\n",
    "&= \\Omega^T((\\mathbb{P}_n(\\Omega \\bf{X})(\\Omega \\bf{X})^T)^{-1} \\mathbb{P}_n(\\Omega \\bf{X})Y \\\\ \n",
    "&= \\Omega^T( \\Omega \\mathbb{P}_n \\bf{X} \\textbf{X}^T \\Omega^T)^{-1}  \\Omega \\mathbb{P}_n  {\\textbf X} Y\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Now set $\\Sigma = \\mathbb{P}_n \\bf{X} \\bf{X}^T$ and $\\Gamma = \\mathbb{P}_n  \\bf{X} Y$. If our data is very large, say 10 million rows, we can make a single pass through the data to compute these quantities, with the below pseudo code:\n",
    "\n",
    "```\n",
    "// initialize\n",
    "X = data\n",
    "Y = target\n",
    "B = number of replicates, chosen by the user\n",
    "n = length(target)\n",
    "Sigma = 0\n",
    "Gamma = 0\n",
    "Beta = 0\n",
    "\n",
    "// Loop through and sum\n",
    "// The goal here is to calculate P_n XX^T and P_n XY exactly once and store them\n",
    "For i in 1 : n\n",
    "\n",
    "    // Calculate XX^T and XY\n",
    "    temp_data = X[i]\n",
    "    temp_target = Y[i]\n",
    "    temp_Sigma = temp_data * temp_data^T\n",
    "    temp_Gamma = temp_data * temp_target\n",
    "    \n",
    "    Sigma = Sigma + temp_Sigma\n",
    "    Gamma = Gamma + temp_Gamma\n",
    "    \n",
    "Sigma = (1/n) * Sigma\n",
    "Gamma = (1/n) * Gamma\n",
    "\n",
    "// Now loop through our B and calculate beta_hat\n",
    "For b in 1 : B\n",
    "    Omega = (k * p matrix randomly sampled from Q)\n",
    "    Inter = Omega @ Sigma @ Omega^T\n",
    "    Inv_inter = Inter^{-1}\n",
    "    projected_beta = Omega^T @ Inv_inter @ Omega @ Gamma\n",
    "    \n",
    "    Beta = Beta + projected_beta\n",
    "\n",
    "Beta = (1/B) * Beta\n",
    "\n",
    "// RETURN Beta\n",
    "```\n",
    "\n",
    "Regardless of the size of our dataset, we calculate $\\Sigma$ and $\\Gamma$ by iterating through the data once, taking $\\bf{X}\\bf{X}^T \\in \\mathbb{R}^{p \\times p}$ and $\\bf{X}\\bf{Y} \\in \\mathbb{R}^{p \\times 1}$ for each $\\bf{X}_i, Y_i$ in the dataset, and then adding it to the current $\\Sigma$ and $\\Gamma$ respectively. Because of this, at any given time, we only store $\\Sigma, \\Gamma, \\bf{X_i}\\bf{X_i}^T, \\bf{X_i}Y_i$, all of which require either $O(p^2)$ or $O(p)$ storage. \n",
    "\n",
    "Once we have $\\Sigma$ and $\\Gamma$, regardless of the size of $B$, we can calculate $\\hat{\\beta}_n^{\\Omega^{(b)}} \\in R^{p \\times 1}$, and then take a rolling sum of $\\hat{\\beta}_n^{\\Omega^{(b)}}$, before taking the average and returning $\\hat{\\beta}^{ave}_n$. Storing each $\\hat{\\beta}_n^{\\Omega^{(b)}}$ has storage $O(p)$, so the algorithm in total requires storage of $O(p^2)$, as desired. We do need to use storage $O(k^2)$ in order to store $\\Omega\\mathbb{P}_n\\mathbf{X}\\mathbf{X}^T\\Omega^T$ in order to take its inverse, but that storage is temporary.\n",
    "\n",
    "<!-- https://users.csc.calpoly.edu/~jdalbey/SWE/pdl_std.html -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634b33ac",
   "metadata": {},
   "source": [
    "### Question 3 {-}\n",
    "\n",
    "For our implementation of this function, we consider $P$ to be a uniform distribution across the integers $\\{k_{min}, k_{min}+1, ..., k_{max}\\}$ by default, but allow the user to pass in some other callable sampling function that will sample over the same range. Likewise, $Q$ defaults to a $N(0,1)$ distribution, but Q can be any callable function that takes in two arguments $(k^{(b)}, p)$ and gives a $k^{(b)} \\times p$ matrix of randomly sampled values. For this implementation, this is most easily done with a `lambda` function on a NumPy function for a sampling distribution with the correct parameters (mean of 0, variance of 1), with the `lambda` function arguments for the size.\n",
    "\n",
    "**We are then also supposed to run a simulation showing our code works effectively (I didn't do this yet).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "92273d57-b80e-427e-b42c-eae539b8c342",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Randy(X, y, B = 1000, Q = lambda x,y: np.random.default_rng(seed = 19).normal(0, 1, (x,y)), k_min = 1, k_max = 100, P = None):\n",
    "    \n",
    "    n = len(y)\n",
    "    p = X.shape[1]\n",
    "    \n",
    "    Sigma = np.zeros(shape = (p, p))\n",
    "    Gamma = np.zeros(shape = (p, 1))\n",
    "    Beta = np.zeros(shape = (p, 1))\n",
    "\n",
    "    #First, we need to calculate Sigma and Gamma\n",
    "    for i in range(0, n):\n",
    "        data_temp = np.asarray(X.iloc[i,]).reshape(-1,1)\n",
    "        targ_temp = y[i]\n",
    "        Sigma_temp = data_temp @ data_temp.T\n",
    "        Gamma_temp = data_temp * targ_temp\n",
    "        \n",
    "        Sigma = Sigma + Sigma_temp\n",
    "        Gamma = Gamma + Gamma_temp\n",
    "    \n",
    "    Sigma = (1/n) * Sigma\n",
    "    Gamma = (1/n) * Gamma\n",
    "    \n",
    "    #Now we need to calculate our B estimates for Beta\n",
    "    for i in range(0, B):\n",
    "    \n",
    "        #Sample k_b from P(k)\n",
    "        if P is not None:\n",
    "            k_b = P(k_min, k_max)\n",
    "        else:\n",
    "            k_b = random.randint(k_min, k_max)\n",
    "            \n",
    "        Omega = Q(k_b, p)\n",
    "        Inter = Omega @ Sigma @ Omega.T\n",
    "        proj_beta = Omega.T @ np.linalg.inv(Inter) @ Omega @ Gamma\n",
    "        \n",
    "        Beta = Beta + proj_beta\n",
    "        \n",
    "    Beta_ave = (1/B) * Beta\n",
    "    \n",
    "    return Beta_ave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c3ade59d-4406-4784-96c1-49b3304b43c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = len(y)\n",
    "p = X.shape[1]\n",
    "\n",
    "Sigma = np.zeros(shape = (p, p))\n",
    "Gamma = np.zeros(shape = (p, 1))\n",
    "Beta = np.zeros(shape = (p, 1))\n",
    "data_temp = np.asarray(X.iloc[0,]).reshape(-1,1)\n",
    "targ_temp = y[0]\n",
    "Sigma_temp = data_temp @ data_temp.T\n",
    "Gamma_temp = data_temp * targ_temp\n",
    "\n",
    "Sigma = Sigma + Sigma_temp\n",
    "Gamma = Gamma + Gamma_temp\n",
    "\n",
    "Sigma = Sigma\n",
    "Gamma = Gamma\n",
    "\n",
    "#Now we need to calculate our B estimates for Beta\n",
    "k_b = random.randint(1, 100)\n",
    "\n",
    "Omega = Q(k_b, p)\n",
    "Inter = Omega @ Sigma @ Omega.T\n",
    "proj_beta = Omega.T @ np.linalg.inv(Inter) @ Omega @ Gamma\n",
    "\n",
    "Beta = Beta + proj_beta\n",
    "        \n",
    "Beta_ave = Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ef9c9af4-5893-451b-8cc3-3d806a3581d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -425984.],\n",
       "       [ -638976.],\n",
       "       [  167936.],\n",
       "       [ -131072.],\n",
       "       [ -294912.],\n",
       "       [  163840.],\n",
       "       [   98304.],\n",
       "       [ -262144.],\n",
       "       [-1376256.],\n",
       "       [-1048576.]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Beta_ave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2b962d43-6a4c-4ad5-bee1-9b825bf89107",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nr.seed(1)\n",
    "X, y = datasets.load_diabetes(return_X_y=True)\n",
    "X = pd.DataFrame(X)\n",
    "Beta_ave = Randy(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "789ab52a-c90a-411d-8fcb-c1b6a68418a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-14203.2886906 ],\n",
       "       [ 15136.02992941],\n",
       "       [  8126.60768972],\n",
       "       [-10481.28728107],\n",
       "       [ 23587.25037792],\n",
       "       [   187.73835719],\n",
       "       [ -8227.22052038],\n",
       "       [-21307.59044041],\n",
       "       [  8704.26323363],\n",
       "       [  1907.08856084]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Beta_ave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1ad6bd2d-f2e4-4d7b-bf26-afa608ed3566",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -10.0098663 , -239.81564367,  519.84592005,  324.3846455 ,\n",
       "       -792.17563855,  476.73902101,  101.04326794,  177.06323767,\n",
       "        751.27369956,   67.62669218])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regressor = LinearRegression(fit_intercept=False)\n",
    "regressor.fit(X, y)\n",
    "\n",
    "regressor.coef_ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb9e5b9",
   "metadata": {},
   "source": [
    "### Question 4 {-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d634b18c-480c-4b4f-9b8d-2ddbdd9e728b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Simulation             Method            fn       MSE\n",
      "0            1  Random Projection  3.139564e+10  0.930972\n",
      "1            1    K-dim Embedding  3.465859e+07  0.940439\n",
      "2            2  Random Projection  2.506777e+10  1.013837\n",
      "3            2    K-dim Embedding  3.485318e+07  1.009961\n",
      "4            3  Random Projection  3.205881e+10  1.060845\n",
      "..         ...                ...           ...       ...\n",
      "195         98    K-dim Embedding  3.467891e+07  1.002303\n",
      "196         99  Random Projection  2.920720e+10  1.007657\n",
      "197         99    K-dim Embedding  3.489242e+07  1.021481\n",
      "198        100  Random Projection  3.289398e+10  1.001732\n",
      "199        100    K-dim Embedding  3.485261e+07  1.001526\n",
      "\n",
      "[200 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "\n",
    "'''''\n",
    "Inputs:\n",
    "Omega: projection matrix of dimension (k,p) used to transform the original X.\n",
    "X: original high-dimensional data matrix. \n",
    "\n",
    "Functions:\n",
    "fn: defines de objective function evaluated on X and k.\n",
    "k_dim_embedding: take top 5 eigenvectors (those with most variation).\n",
    "genera_data: creates a synthetic dataset given some paramters.\n",
    "\n",
    "Output:\n",
    "results_df: pd df that contains, per each iteration and each method, the respective value of fn and of k-dim embdedding.\n",
    "\n",
    "'''''\n",
    "\n",
    "# Objective function fn\n",
    "def fn(Omega, X):\n",
    "    n, p = X.shape\n",
    "    X_Omega = X @ Omega.T\n",
    "    inner_products = X @ X.T\n",
    "    Omega_inner_products = X_Omega @ X_Omega.T\n",
    "    return np.sum((inner_products - Omega_inner_products)**2)\n",
    "\n",
    "# K-dim embedding function\n",
    "def k_dim_embedding(X, k):\n",
    "    n, p = X.shape\n",
    "    # Covariance matrix\n",
    "    cov_X = np.cov(X.T)\n",
    "    # Top k eigenvectors of covariance\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(cov_X)\n",
    "    # Take k eigenvectors corresponding to the k largest eigenvalues\n",
    "    Omega_kdim = eigenvectors[:, -k:]\n",
    "    # Return Omega as a (k, p) matrix\n",
    "    return Omega_kdim.T\n",
    "\n",
    "# Parameters\n",
    "n = 1000 # sample size\n",
    "p = 50 # number of dimensions/regressors\n",
    "k = 10 # number of reduced dimensions\n",
    "n_simulations = 100  # number of simulations\n",
    "\n",
    "# Results dataframe\n",
    "results_df = pd.DataFrame(columns=['Simulation', 'Method', 'fn', 'MSE'])\n",
    "\n",
    "for i in range(n_simulations):\n",
    "    # Generate  data\n",
    "    X = np.random.normal(size=(n, p))\n",
    "    y = np.random.normal(size=n)\n",
    "    \n",
    "    # Calculate fn and MSE for random projection method\n",
    "    # We choose random normal \n",
    "    Omega_random = np.random.normal(size=(k, p))\n",
    "    fn_random = fn(Omega_random, X)\n",
    "    model_random = LinearRegression()\n",
    "    X_random = X @ Omega_random.T\n",
    "    model_random.fit(X_random, y)\n",
    "    y_pred_random = model_random.predict(X_random)\n",
    "    mse_random = np.mean((y - y_pred_random)**2)\n",
    "    results_df = pd.concat([results_df, pd.DataFrame({'Simulation': [i+1], 'Method': ['Random Projection'], 'fn': [fn_random], 'MSE': [mse_random]})])\n",
    "    \n",
    "    # Calculate fn and MSE for k-dim embedding method\n",
    "    Omega_kdim = k_dim_embedding(X, k)\n",
    "    fn_kdim = fn(Omega_kdim, X)\n",
    "    model_kdim = LinearRegression()\n",
    "    X_kdim = X @ Omega_kdim.T\n",
    "    model_kdim.fit(X_kdim, y)\n",
    "    y_pred_kdim = model_kdim.predict(X_kdim)\n",
    "    mse_kdim = np.mean((y - y_pred_kdim)**2)\n",
    "    results_df = pd.concat([results_df, pd.DataFrame({'Simulation': [i+1], 'Method': ['K-dim Embedding'], 'fn': [fn_kdim], 'MSE': [mse_kdim]})])\n",
    "    \n",
    "results_df.reset_index(drop=True, inplace=True)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39da40a3",
   "metadata": {},
   "source": [
    "### Resources and Notes {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1175b770",
   "metadata": {},
   "source": [
    "1. "
   ]
  }
 ],
 "metadata": {
  "date": "February 20, 2023",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "title": "STA 561 Homework 5"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
